{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buzz Words but What do They Mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NNs are a small subset of ai, ml, and deep learning](https://github.com/epochml/epoch_intersession_2020-2021/blob/master/images/what_is_a_neural_network/small_subset_of_ai.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Network\n",
    "\n",
    "![simple neural network architecture](https://github.com/epochml/epoch_intersession_2020-2021/blob/master/images/what_is_a_neural_network/basic_neural_network.jpeg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are really a tiny subset of a bunch of different, larger categories of problem-solving techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![basic neuron in a neural network](https://github.com/epochml/epoch_intersession_2020-2021/blob/master/images/what_is_a_neural_network/basic_neuron_with_bias.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic unit of a neural network is a single neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the Parts of a Neuron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. inputs\n",
    "2. weight & bias\n",
    "3. sum (can be expressed as the dot product)\n",
    "4. activation function \n",
    "5. output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The basic Neuron class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNeuron:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def calc_neuron_output(self, inputs):\n",
    "        sum = 0\n",
    "        for inpt, weight in zip(inputs, self.weights):\n",
    "            sum += inpt * weight\n",
    "        sum += self.bias\n",
    "        return sum \n",
    "    \n",
    "        ###############\n",
    "        ## IMPORTANT ##\n",
    "        ###############\n",
    "        # there is a much better way to do this calculation, which we will talk about. This is just for instructional purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our basic Neuron class contains a few methods and a few attributes. \n",
    "\n",
    "It has a constructor which allows us to create Neurons.\n",
    "\n",
    "It has a calc_neuron_output function (function and method mean the same thing) which takes in some inputs and performs the following calculation: \n",
    "\n",
    "\\begin{equation*}\n",
    "y_j = b_j +  \\sum_{i} x_iw_{ij}\n",
    "\\end{equation*}\n",
    "\n",
    "Which means the output of neuron \"y sub j\" is the sum of all the neuron's inputs times their respective weights plus a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = BasicNeuron()\n",
    "n.weights = [0.1, 0.2, 0.3, 0.4]\n",
    "n.bias = 1\n",
    "inpts = [1, 1, 1, 1]\n",
    "\n",
    "n.calc_neuron_output(inpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our basic Neuron class works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting a layer together\n",
    "\n",
    "A \"layer\" is composed of many neurons, each with their own weights and biases.\n",
    "\n",
    "The benefit of thinking of our neural network in terms of layers is it simplifies a lot of the calculations we must do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![types of activation functions](https://github.com/epochml/epoch_intersession_2020-2021/blob/master/images/what_is_a_neural_network/dot_product_representation.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the dot product instead of calculting the output for each and every neuron makes things so much easier and more efficient. \n",
    "\n",
    "Actually, we don't even need our BasicNeuron class if we use the dot product instead of calculating the output of each neuron one-by-one. All we need to consider is the ENTIRE layer and all of the weights and biases that belong to it.\n",
    "\n",
    "Lets make things simpler by making a layer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import codecs\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, number_input_neurons = 0, number_output_neurons = 0, weights = np.array([]), biases = np.array([])):\n",
    "        self.number_input_neurons = number_input_neurons\n",
    "        self.number_output_neurons = number_output_neurons\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        \n",
    "    def initalize_random_weights(self):\n",
    "        self.weights = np.random.rand(self.number_output_neurons, self.number_input_neurons) - 0.5\n",
    "    \n",
    "    def initalize_random_biases(self):\n",
    "        self.biases = np.random.rand(self.number_output_neurons, 1) - 0.5\n",
    "        \n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.weights, self.input) + self.biases\n",
    "        return self.output\n",
    "    \n",
    "    # We'll explain this in a bit! For now know that this is important for later.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(self.weights.T, output_error)\n",
    "        weights_error = np.dot(output_error, self.input.T)\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.biases -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = Layer(2, 2)\n",
    "layer.initalize_random_weights()\n",
    "layer.initalize_random_biases()\n",
    "print(\"weights:\\n{}\".format(layer.weights))\n",
    "print(\"\\n##########\\n\")\n",
    "print(\"baises:\\n{}\".format(layer.biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[0,0],\n",
    "                   [0,1],\n",
    "                   [1,0],\n",
    "                   [1,1]])\n",
    "\n",
    "expected_outputs = np.array([[0], \n",
    "                             [1], \n",
    "                             [1], \n",
    "                             [0]])\n",
    "\n",
    "inputs = np.reshape(inputs, (4,-1,1))\n",
    " # rotates the entire array. We need to rotate it because we wanted the input to our NN make visual sense.\n",
    " # In most cases, you would not have to do this, but it might aid in understanding to see it this way\n",
    "     #[[[0],\n",
    "     # [0]],\n",
    "     #\n",
    "     #[[0],\n",
    "     #[1]],\n",
    "     #\n",
    "     #[[1],\n",
    "     #[0]],\n",
    "     #\n",
    "     #[[1],\n",
    "     #[1]]]\n",
    "\n",
    "for inpt in inputs:\n",
    "    print(layer.forward_propagation(inpt))\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a few activation functions](https://github.com/epochml/epoch_intersession_2020-2021/blob/master/images/what_is_a_neural_network/types_of_activation_functions.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sigmoid Activation Function\n",
    "This is what we'll be using for this simple example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![the sigmoid function](https://github.com/epochml/epoch_intersession_2020-2021/blob/master/images/what_is_a_neural_network/sigmoid.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "# we'll need the derivative of this function later!\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy allows us to apply a function to every value in the array\n",
    "activation = sigmoid(layer.output)\n",
    "\n",
    "print(layer.output)\n",
    "print(\"\\n#######\\n\")\n",
    "print(\"after applying the sigmoid function to every value in the array:\\n\\n{}\".format(activation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will often see these activation functions being included in their own layers called the \"activation layer.\"\n",
    "\n",
    "Lets make an activation layer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer(Layer):\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "        self.input = None\n",
    "        \n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = sigmoid(self.input)\n",
    "        return self.output\n",
    "    \n",
    "    # Stay with us with all this backward propagation stuff. It'll make more sense in a moment.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return sigmoid_derivative(self.input) * output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = ActivationLayer()\n",
    "activation.forward_propagation(layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add another layer to our network with two inputs and one output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2 = Layer(2, 1)\n",
    "layer_2.initalize_random_weights()\n",
    "layer_2.initalize_random_biases()\n",
    "print(\"weights:\\n{}\".format(layer_2.weights))\n",
    "print(\"\\n##########\\n\")\n",
    "print(\"baises:\\n{}\".format(layer_2.biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2.forward_propagation(activation.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works just about how you would expect it to!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a basic Layer class which takes some inputs, has some weights and biases, and helps us \"train\" our neural network. We've also made an activation layer class that applies an activation function to all the data we pass into it and will also help us \"train\" our NN.\n",
    "\n",
    "You might be wondering what it means to \"train\" a NN, and that is a very important question. \n",
    "\n",
    "A neural network works because we can use a little bit of calculus to adjust the weights and biases in a smart way that allows us to make our predictions more accurate. This process of adjusting the weights and biases is what it means to \"train\" a NN. It is ok if you are not super comfortable with the calculus, it is not super important to understand fully right now, but it might give you a better intuition into exactly what makes neural networks work. We'll keep things as simple as we can so let's dive into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to adjusting our weights and biases is understanding how off our prediction was. After we understand how off our prediction was, we can start talking about how we can go about making our neural network perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions (Cost Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can measure the error in our prediction using what is called a loss function (also known as a cost function). We will be using the Mean Squared Error function for this example\n",
    "![MSE](https://github.com/epochml/epoch_intersession_2020-2021/blob/master/images/what_is_a_neural_network/mean_squared_error.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take the two arrays and calculate the mse.\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "# we'll need this later too!\n",
    "def mean_squared_error_derivative(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a measure of how off our predictions are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of Back Propogation (backprop) is pretty simple: adjust the weights in the network in proportion to how much each neuron contributes to the overall error. Through optimization with gradient descent (which will be explained later), the weights will be iteratively changed so that the loss is minimized. \n",
    "\n",
    "Since forward propogation is plugging into a formula that is effectively y=mx+b but nested, back propogation is reversing that process using calculus to determine the derivative w.r.t a variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a forward prop:\n",
    "\n",
    "f(x) = A(B(C(x)))\n",
    "\n",
    "with A, B, and C being the activation functions, we can find neuron B's impact on the overall error by finding df/dB, or f'(B), or the derivative of f(x) w.r.t B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is an optimization method to minimize the loss after each cycle of backprop. If you have taken calculus before, you know that the derivative of the function will point towards the local minima. We compute the derivative with a small step size, or small run, to find the weights for which the loss is the lowest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gd_1](https://github.com/epochml/epoch_intersession_2020-2021/blob/master/images/what_is_a_neural_network/gradient_descent.png?raw=true)\n",
    "\n",
    "![gd_2](https://github.com/epochml/epoch_intersession_2020-2021/blob/master/images/what_is_a_neural_network/gradient_descent_demystified.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the parts of our NN, We can put them all together in a nice little class to make things easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "        \n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def predict(self, input_data):\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        for data in input_data:\n",
    "            output = data\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def fit(self, x_train, y_train, no_epochs, learning_rate):\n",
    "        for i in range(no_epochs):\n",
    "            error = 0\n",
    "            for counter, j in enumerate(x_train):\n",
    "                output = j\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "                \n",
    "                error += mean_squared_error(y_train[counter], output)\n",
    "                \n",
    "                # backprop\n",
    "                \n",
    "                err = mean_squared_error_derivative(y_train[counter], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    err = layer.backward_propagation(err, learning_rate)\n",
    "                    \n",
    "            error /= len(x_train)\n",
    "            print(\"epoch {}  error = {}\".format(i, error))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork()\n",
    "first_layer = Layer(2,2)\n",
    "first_layer.initalize_random_weights()\n",
    "first_layer.initalize_random_biases()\n",
    "second_layer = Layer(2,1)\n",
    "second_layer.initalize_random_weights()\n",
    "second_layer.initalize_random_biases()\n",
    "\n",
    "print(first_layer.forward_propagation(np.array([[0],[0]])))\n",
    "\n",
    "net.add(first_layer)\n",
    "net.add(ActivationLayer())\n",
    "net.add(second_layer)\n",
    "net.add(ActivationLayer())\n",
    "\n",
    "net.fit(inputs, expected_outputs, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = net.predict(inputs)\n",
    "for inpt, expected, actual in zip(inputs, expected_outputs, predictions):\n",
    "    print(\"input: {}, expected: {}, result: {}\".format([int(inpt[0]), int(inpt[1])], expected, actual))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! Our Hand-coded neural network has correctly learned how to classify our inputs into the desired outputs! Maybe this doesn't seem like a huge deal, but lets scale up the project a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MNIST Dataset\n",
    "\n",
    "The MNIST dataset is very famous dataset that contains a bunch of labled hand-written digits. Lets use the same classes we created to help us classify these digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset in included in our \"data\" folder, but it can also be downloaded from [this link](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the image files and labels of the MNIST dataset is encoded into these 4 files. We need to be able to extract the images from the files to work with them.\n",
    "\n",
    "### File descriptions\n",
    "Four files are provided (the .gz files are zipped versions of these files):\n",
    "\n",
    "* Test Images : t10k-images-idx3-ubyte\n",
    "* Test Labels :  t10k-labels-idx1-ubyte\n",
    "* Train Images : train-images-idx3-ubyte\n",
    "* Train Labels :  train-labels-idx1-ubyte\n",
    "\n",
    "The IDX file format is a simple format for vectors and multidimensional matrices of various numerical types.\n",
    "\n",
    "#### The basic format for labels\n",
    "  \n",
    "|Offset | Type               | Value           |   Description                   |\n",
    "|-------|--------------------|-----------------|---------------------------------|\n",
    "|0000   |4 byte integer      |0x00000801(2049) |magic number (MSB first)         |\n",
    "|0004   |4 byte integer      |10000 or 60000   |number of items (test or train)  |\n",
    "|0008   |unsigned byte       |??               |label                            |\n",
    "|0009   |unsigned byte       |??               |label                            |\n",
    "|...    |...                 |...              |...                              |\n",
    "|xxxx   |unsigned byte       |??               |label                            |\n",
    "\n",
    "\n",
    "#### The basic format for images\n",
    "\n",
    "|Offset | Type               | Value           |   Description                   |\n",
    "|-------|--------------------|-----------------|---------------------------------|\n",
    "|0000   |4 byte integer      |0x00000801(2051) |magic number (MSB first)         |\n",
    "|0004   |4 byte integer      |10000 or 60000   |number of images (test or train) |\n",
    "|0008   |4 byte integer      |28               |number of rows                   |\n",
    "|0012   |4 byte integer      |28               |number of columns                |\n",
    "|0016   |unsigned byte       |??               |pixel intensity (0-255)          |\n",
    "|0017   |unsigned byte       |??               |pixel intensity (0-255)          |\n",
    "|...    |...                 |...              |...                              |\n",
    "|xxxx   |unsigned byte       |??               |pixel intensity (0-255)          |\n",
    "\n",
    "\n",
    "### Converting the ubyte files to numpy arrays for easy processing\n",
    "The following code converts the ubyte files into four numpy n dimensional arrays and stores them in a dictionary called `data_dict` which has four key, value pairs.\n",
    "\n",
    "| Key           |  Type        |Shape         |\n",
    "|---------------|--------------|--------------|\n",
    "|*train_images* |numpy ndarray |[60000,28,28] |\n",
    "|*train_labels* |numpy ndarray |[60000]       |\n",
    "|*test_images*  |numpy ndarray |[10000,28,28] |\n",
    "|*test_labels*  |numpy ndarray |[10000]       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files # this only works using google colabs\n",
    "files = files.upload()\n",
    "\n",
    "# PROVIDE YOUR DIRECTORY WITH THE EXTRACTED FILES HERE\n",
    "# datapath = '../data/MNIST/raw/uncompressed/'\n",
    "\n",
    "# files = os.listdir(datapath)\n",
    "# print(files)\n",
    "\n",
    "def get_int(b):   # CONVERTS 4 BYTES TO A INT\n",
    "    return int(codecs.encode(b, 'hex'), 16)\n",
    "\n",
    "data_dict = {}\n",
    "for file in files:\n",
    "    if file.endswith('ubyte'):  # FOR ALL 'ubyte' FILES\n",
    "        print('Reading ',file)\n",
    "        # with open (datapath+file,'rb') as f:\n",
    "        with open (file,'rb') as f:\n",
    "            data = f.read()\n",
    "            type = get_int(data[:4])   # 0-3: THE MAGIC NUMBER TO WHETHER IMAGE OR LABEL\n",
    "            length = get_int(data[4:8])  # 4-7: LENGTH OF THE ARRAY  (DIMENSION 0)\n",
    "            if (type == 2051):\n",
    "                category = 'images'\n",
    "                num_rows = get_int(data[8:12])  # NUMBER OF ROWS  (DIMENSION 1)\n",
    "                num_cols = get_int(data[12:16])  # NUMBER OF COLUMNS  (DIMENSION 2)\n",
    "                parsed = np.frombuffer(data,dtype = np.uint8, offset = 16)  # READ THE PIXEL VALUES AS INTEGERS\n",
    "                parsed = parsed.reshape(length,num_rows,num_cols)  # RESHAPE THE ARRAY AS [NO_OF_SAMPLES x HEIGHT x WIDTH]           \n",
    "            elif(type == 2049):\n",
    "                category = 'labels'\n",
    "                parsed = np.frombuffer(data, dtype=np.uint8, offset=8) # READ THE LABEL VALUES AS INTEGERS\n",
    "                parsed = parsed.reshape(length)  # RESHAPE THE ARRAY AS [NO_OF_SAMPLES]                           \n",
    "            if (length==10000):\n",
    "                set = 'test'\n",
    "            elif (length==60000):\n",
    "                set = 'train'\n",
    "            data_dict[set+'_'+category] = parsed  # SAVE THE NUMPY ARRAY TO A CORRESPONDING KEY   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dict[\"train_images\"].shape)\n",
    "print(data_dict[\"train_labels\"].shape)\n",
    "print(data_dict[\"test_images\"].shape)\n",
    "print(data_dict[\"test_labels\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make a NN that'll help us classify these digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = NeuralNetwork()\n",
    "\n",
    "input_layer = Layer(28*28, 1000)\n",
    "input_layer.initalize_random_weights()\n",
    "input_layer.initalize_random_biases()\n",
    "mnist_net.add(input_layer)                \n",
    "\n",
    "mnist_net.add(ActivationLayer())\n",
    "\n",
    "first_hidden_layer = Layer(1000, 50)\n",
    "first_hidden_layer.initalize_random_weights()\n",
    "first_hidden_layer.initalize_random_biases()\n",
    "mnist_net.add(first_hidden_layer)   \n",
    "\n",
    "mnist_net.add(ActivationLayer())\n",
    "\n",
    "second_hidden_layer = Layer(50, 10)\n",
    "second_hidden_layer.initalize_random_weights()\n",
    "second_hidden_layer.initalize_random_biases()\n",
    "mnist_net.add(second_hidden_layer)   \n",
    "\n",
    "mnist_net.add(ActivationLayer())\n",
    "\n",
    "# 784 -> 100 -> activation -> 50 -> activation -> 10 -> activation\n",
    "\n",
    "reshaped_train_image_data = np.reshape(data_dict[\"train_images\"], (60000, 28*28, 1))\n",
    "reshaped_train_label_data = np.reshape(data_dict[\"train_labels\"], (60000, -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a little complex, but in order to change our labels from a single value like 1 or 2 to an array\n",
    "# like [0,0,1,0,0,0,0,0,0,0], we need to create a \"one-hot\" vector/array.\n",
    "def one_hot(classes, data):\n",
    "    vector = np.zeros(classes)\n",
    "    vector[int(data)] = 1\n",
    "    vector = np.reshape(vector, (-1,1))\n",
    "    return vector\n",
    "    \n",
    "reshaped_train_label_data_one_hot = []\n",
    "for data in reshaped_train_label_data:\n",
    "    reshaped_train_label_data_one_hot.append(one_hot(10, data))\n",
    "    \n",
    "\n",
    "reshaped_train_label_data = np.array(reshaped_train_label_data_one_hot)\n",
    "\n",
    "shuffler = np.random.permutation(len(reshaped_train_label_data))\n",
    "reshaped_train_image_data = reshaped_train_image_data[shuffler]\n",
    "reshaped_train_label_data = reshaped_train_label_data[shuffler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we divide by 255 to normalize the data i.e. making all the data between 0 and 1 instead of between 0 and 255\n",
    "mnist_net.fit((reshaped_train_image_data / 255)[:1000], (reshaped_train_label_data)[:1000], 50, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_test_image_data = np.reshape(data_dict[\"test_images\"], (10000, 28*28, 1))\n",
    "reshaped_test_label_data = np.reshape(data_dict[\"test_labels\"], (10000, -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_test_label_data_one_hot = []\n",
    "for data in reshaped_test_label_data:\n",
    "    reshaped_test_label_data_one_hot.append(one_hot(10, data))\n",
    "    \n",
    "\n",
    "reshaped_test_label_data = np.array(reshaped_test_label_data_one_hot)\n",
    "\n",
    "shuffler = np.random.permutation(len(reshaped_train_label_data))\n",
    "reshaped_test_image_data = reshaped_train_image_data[shuffler]\n",
    "reshaped_test_label_data = reshaped_train_label_data[shuffler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = mnist_net.predict(reshaped_test_image_data)\n",
    "wrong = 0\n",
    "for count, output in enumerate(outputs):\n",
    "    index = np.argmax(output)\n",
    "    true_index = np.argmax(reshaped_test_label_data[count])\n",
    "    print(index, true_index)\n",
    "    if index != true_index:\n",
    "        wrong += 1\n",
    "        \n",
    "print(\"percent wrong: {}%\".format(100 * wrong / len(reshaped_test_image_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we've done it. There are ways to get much higher accuracies, but we've proved that our code does what we want it to do. It can, with relatively ok accuracy, classifiy hand-written digits! Tomorrow we'll talk about another very common technique that can do this task with orders of magnitude of more accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
